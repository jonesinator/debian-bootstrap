#!/bin/sh
set -eu

# Debian mirror configuration.
: "${debian_mirror_url:="http://deb.debian.org/debian"}"
: "${debian_mirror_distribution:="bookworm"}"
: "${debian_mirror_components:="main"}"
: "${debian_mirror_key_url:="https://ftp-master.debian.org/keys"}"
: "${debian_mirror_keys:="archive-key-11.asc archive-key-12.asc release-12.asc"}"
: "${debian_mirror_fingerprints:="1F89983E0081FDE018F3CC9673A4F27B8DD47936"
                                 "B8B80B5B623EAB6AD8775C45B7C5D7D6350947F8"
                                 "4D64FEC119C2029067D6E791F8D2585B8783D481"}"

# Verification configuration.
: "${keyserver:="hkps://keyserver.ubuntu.com"}"

# Local directory configuration.
: "${cache_dir:="./cache"}"
: "${temp_dir:="./tmp"}"
: "${output_dir:="./rootfs"}"

#
# Functions
#

# Executes a command in a target directory as the root directory.
in_target() {
    env -i PATH="/usr/bin:/usr/sbin" DEBIAN_FRONTEND=noninteractive chroot "${@}"
}

# Takes two lists as arguments, and removes any line in the second from the
# first. The result is output on stdout.
remove_from() {
    printf "%s\n%s\n%s" "${1}" "${2}" "${2}" | sort | uniq -u
}

# Determines the path in the local cache to a file by its URL. Prints the path
# to the file on stdout.
cache_path() {
    url__a="${1}"
    cache_dir__a="${2}"

    host__a="$(echo "${url__a}" | sed 's|^https\?://||' | cut -d/ -f1)"
    file_path__a="$(echo "${url__a}" | sed 's|^https\?://||' | cut -d/ -f2-)"
    echo "${cache_dir__a}/${host__a}/${file_path__a}" |
        sed 's/%/\\\\x/g' | xargs echo -e

    unset url__a cache_dir__a host__a file_path__a
}

# Downloads a file by URL to the local cache and prints the path to the file on
# stdout. Any existing file will not be overwritten, and if the file exists then
# this function will not attempt to reach out to the internet. This function
# should be safe to use in offline contexts assuming the cache is populated.
#
# ALL access to the internet should be mediated through this function, and this
# function alone.
download() {
    url__b="${1}"
    cache_dir__b="${2}"

    wget -q --show-progress -nc --tries 5 --waitretry 30 "${url__b}" \
        -P "$(dirname "$(cache_path "${url__b}" "${cache_dir__b}")")"
    cache_path "${url__b}" "${cache_dir__b}"

    unset url__b cache_dir__b
}

# Prints the contents of the "InRelease" file of the package archive after
# validating its signatures. The contents of the signed "InRelease" file and the
# public keys used to validate it are cached in the cache directory.
get_release() {
    mirror__c="${1}"
    distribution__c="${2}"
    key_url__c="${3}"
    keys__c="${4}"
    fingerprints__c="${5}"
    keyserver__c="${6}"
    cache_dir__c="${7}"
    temp_path__c="${8}"

    # Download the signing keys and the InRelease file, then validate the
    # InRelease file.
    rm -rf "${temp_path__c}"
    mkdir -m 0700 "${temp_path__c}"
    for key__c in ${keys__c}; do
        key_path__c="$(download "${key_url__c}/${key__c}" "${cache_dir__c}")"
        gpg --homedir "${temp_path__c}" --import "${key_path__c}"
    done
    in_release_path__c="$(download \
        "${mirror__c}/dists/${distribution__c}/InRelease" "${cache_dir__c}")"
    gpg --homedir "${temp_path__c}" --verify "${in_release_path__c}"
    rm -rf "${temp_path__c}"

    # Validate the InRelease file using the fingerprints and a keyserver, if
    # requested.
    if [ "${keyserver__c}" != "none" ]; then
        mkdir -m 0700 "${temp_path__c}"
        for fingerprint__c in ${fingerprints__c}; do
            gpg --homedir "${temp_path__c}" \
                --keyserver "${keyserver__c}" \
                --recv-keys "${fingerprint__c}"
        done
        gpg --homedir "${temp_path__c}" --verify "${in_release_path__c}"
        rm -rf "${temp_path__c}"
    fi

    cat "${in_release_path__c}"

    unset mirror__c distribution__c key_url__c keys__c fingerprints__c
    unset keyserver__c cache_dir__c temp_path__c key__c in_release_path__c
    unset fingerprint__c
}

# Prints the verified contents a file described by the "Release" file of the
# package archive by path.
get_release_file() {
    mirror__d="${1}"
    distribution__d="${2}"
    release__d="${3}"
    path__d="${4}"
    cache_dir__d="${5}"
    temp_path__d="${6}"

    # Parse the files from the release in the format " {sha256} {size} {path}".
    # The first sed starts capturing after the line with "SHA256:", and the
    # second sed stops capturing on the first line that does not stop with a
    # space.
    files__d="$(echo "${release__d}" | sed '1,/^SHA256:$/ d' | sed '/^[^ ]/Q')"

    # Look for the compressed file.
    for compression__d in "xz" "gz"; do
        packed_path__d="${path__d}.${compression__d}"
        packed_hash__d="$(echo "${files__d}" |
            grep " ${packed_path__d}$" | cut -d' ' -f2)"
        if [ -n "${packed_hash__d}" ]; then
            break
        fi
    done
    if [ -z "${packed_hash__d}" ]; then
        return 1
    fi

    # Prefer getting files by their hash if possible.
    by_hash__d="$( \
        echo "${release__d}" | sed -n 's/Acquire-By-Hash: //p' || echo "no")"
    if [ "${by_hash__d}" = "yes" ]; then
        download_path__d="$(dirname "${path__d}")/by-hash/SHA256/${packed_hash__d}"
    else
        download_path__d="${packed_path__d}"
    fi

    # Get the compressed file and check its hash.
    url__d="${mirror__d}/dists/${distribution__d}/${download_path__d}"
    file__d="$(download "${url__d}" "${cache_dir__d}")"
    echo "${packed_hash__d}  ${file__d}" | sha256sum -c --status

    # Decompress the downloaded file and check its hash as well.
    extension__d="$(echo "${packed_path__d}" | rev | cut -d. -f1 | rev)"
    rm -rf "${temp_path__d}"
    case "${extension__d}" in
        gz) gzip --decompress --stdout "${file__d}" > "${temp_path__d}" ;;
        xz) xz --decompress --stdout "${file__d}" > "${temp_path__d}" ;;
    esac
    unpacked_hash__d="$(echo "${files__d}" | grep "${path__d}$" | cut -d' ' -f2)"
    echo "${unpacked_hash__d}  ${temp_path__d}" | sha256sum -c --status
    cat "${temp_path__d}"
    rm "${temp_path__d}"

    unset mirror__d distribution__d release__d path__d cache_dir__d temp_path__d
    unset files__d compression__d packed_path__d packed_hash__d by_hash__d
    unset download_path__d url__d file__d extension__d unpacked_hash__d
}

# Converts a package metadata file URL to the filename it should have in
# /var/cache/apt. Prints the filename to stdout.
url_to_apt_list() {
    url__e="${1}"

    echo "${url__e}" | sed 's|^https\?://||' | sed 's|/|_|g'

    unset url__e
}

# Runs a "manual" apt update by downloading the appropriate package metadata
# files from the mirror and putting them in the output dir with the correct
# names for /var/cache/apt. Effectially this does an "apt update", but it uses
# this script's cache and download utilities.
manual_apt_update() {
    url__f="${1}"
    distribution__f="${2}"
    components__f="${3}"
    key_url__f="${4}"
    keys__f="${5}"
    fingerprints__f="${6}"
    keyserver__f="${7}"
    cache_dir__f="${8}"
    temp_dir__f="${9}"
    out_dir__f="${10}"
    shift 10
    files__f="${*}"

    prefix__f="${url__f}/dists/${distribution__f}"
    release_data__f="$(get_release \
        "${url__f}" \
        "${distribution__f}" \
        "${key_url__f}" \
        "${keys__f}" \
        "${fingerprints__f}" \
        "${keyserver__f}" \
        "${cache_dir__f}" \
        "${temp_dir__f}" | tee "${out_dir__f}/$(url_to_apt_list "${prefix__f}/InRelease")")"
    for component__f in ${components__f}; do
        for file__f in ${files__f}; do
            contents__f="$(get_release_file \
                "${url__f}" \
                "${distribution__f}" \
                "${release_data__f}" \
                "${component__f}/${file__f}" \
                "${cache_dir__f}" \
                "${temp_dir__f}")"
            if [ -n "${contents__f}" ]; then
                echo "${contents__f}" > "${out_dir__f}/$(url_to_apt_list "${prefix__f}/${component__f}/${file__f}")"
            fi
        done
    done

    touch "${out_dir__f}/lock"
    mkdir -p "${out_dir__f}/auxfiles"
    mkdir -p "${out_dir__f}/partial"

    unset url__f distribution__f components__f key_url__f keys__f
    unset fingerprints__f keyserver__f cache_dir__f temp_dir__f out_dir__f
    unset files__f prefix__f release_data__f component__f file__f contents__f
}

manual_apt_install() {
    cache_dir__g="${1}"
    output_dir__g="${2}"
    command__g="${3}"
    shift 3
    args__g="${*}"

    # shellcheck disable=SC2086
    infos__g="$(in_target "${output_dir__g}" apt-get -qq "${command__g}" --print-uris ${args__g})"
    echo "${infos__g}" | while IFS= read -r info__g; do
      uri__g="$(echo "${info__g}" | cut -d' ' -f1 | tr -d "\'")"
      destination__g="$(echo "${info__g}" | cut -d' ' -f2)"
      md5_hash__g="$(echo "${info__g}" | cut -d' ' -f4 | cut -c8-)"
      cache_path__g="$(download "${uri__g}" "${cache_dir__g}")"
      if [ -n "${md5_hash__g}" ]; then
          echo "${md5_hash__g}  ${cache_path__g}" | md5sum -c --status
      else
          echo "WARNING: Not validating hash for ${destination__g}" 1>&2
      fi
      mkdir -p "${output_dir}/var/cache/apt/archives"
      cp "${cache_path__g}" "${output_dir__g}/var/cache/apt/archives/${destination__g}"
    done
    # shellcheck disable=SC2086
    in_target "${output_dir__g}" apt-get "${command__g}" --no-download --yes ${args__g}

    unset cache_dir__g output_dir__g command__g args__g infos__g uri__g
    unset destination__g md5_hash__g cache_path__g
}

#
# Main Script
#

# Remount (if needed)
umount "${output_dir}/tmp" || true
umount "${output_dir}/dev" || true
umount "${output_dir}/proc" || true
mount -t tmpfs nodev "${output_dir}/tmp"
mount -t proc nodev "${output_dir}/proc"
mount -t devtmpfs nodev "${output_dir}/dev"

echo "deb-src ${debian_mirror_url} ${debian_mirror_distribution} ${debian_mirror_components}" >> "${output_dir}/etc/apt/sources.list"

manual_apt_update \
    "${debian_mirror_url}" \
    "${debian_mirror_distribution}" \
    "${debian_mirror_components}" \
    "${debian_mirror_key_url}" \
    "${debian_mirror_keys}" \
    "${debian_mirror_fingerprints}" \
    "${keyserver}" \
    "${cache_dir}" \
    "${temp_dir}" \
    "${output_dir}/var/lib/apt/lists" \
    "binary-amd64/Packages" "i18n/Translation-en" "source/Sources"

manual_apt_install "${cache_dir}" "${output_dir}" install --yes build-essential

installed_packages="$(in_target "${output_dir}" dpkg --list | awk '/^ii/ { print $2 }')"

# Iteratively get build dependencies until we stop adding new packages.
echo "Geting build dependencies"
build_dep_uris=
packages="${installed_packages}"
while [ -n "${packages}" ]; do
    all_uris=
    for package in $(echo "${packages}" | tr '\n' ' '); do
        uris="$(in_target "${output_dir}" apt-get -qq build-dep --print-uris "${package}")"
        all_uris="$(printf "%s\n%s" "${uris}" "${all_uris}")"
    done
    new_uris="$(remove_from "${all_uris}" "${build_dep_uris}")"
    build_dep_uris="$(printf "%s\n%s" "${build_dep_uris}" "${new_uris}" | sort | uniq)"
    packages="$(echo "${new_uris}" | awk '{print $2}' | cut -f1 -d_ | sort | uniq)"
done

# Now we have a complete list of binary packages that we need (the currently
# installed packages, and their recursive build dependencies). Get the URIs for
# the source packages for all of these binary packages.
echo "Geting source URIs"
build_dep_packages="$(echo "${build_dep_uris}" | awk '{print $2}' | cut -f1 -d_ | sort | uniq)"
all_packages="$(printf "%s\n%s" "${installed_packages}" "${build_dep_packages}" | sort | uniq)"
sources=
for package in $(echo "${all_packages}" | tr "\n" " "); do
    package_sources="$(in_target "${output_dir}" apt-get -qq source --print-uris "${package}")"
    sources="$(printf "%s\n%s" "${package_sources}" "${sources}")"
done
source_uris="$(echo "${sources}" | sort | uniq)"

# Download all of the binaries.
echo "${build_dep_uris}" | while read -r line; do
    [ -z "${line}" ] && continue
    uri="$(echo "${line}" | cut -d' ' -f1 | tr -d "\'")"
    destination="$(echo "${line}" | cut -d' ' -f2)"
    md5_hash="$(echo "${line}" | cut -d' ' -f4 | cut -c8-)"
    cache_path="$(download "${uri}" "${cache_dir}")"
    if [ -n "${md5_hash}" ]; then
        echo "${md5_hash}  ${cache_path}" | md5sum -c --status
    else
        echo "WARNING: Not validating hash for ${destination}" 1>&2
    fi
done

# Download all of the source files.
echo "${source_uris}" | while read -r line; do
    [ -z "${line}" ] && continue
    uri="$(echo "${line}" | cut -d' ' -f1 | tr -d "\'")"
    destination="$(echo "${line}" | cut -d' ' -f2)"
    sha256_hash="$(echo "${line}" | cut -d' ' -f4 | cut -c8-)"
    cache_path="$(download "${uri}" "${cache_dir}")"
    if [ -n "${sha256_hash}" ]; then
        echo "${sha256_hash}  ${cache_path}" | sha256sum -c --status
    else
        echo "WARNING: Not validating hash for ${destination}" 1>&2
    fi
done

echo "${all_packages}" > all-packages

umount "${output_dir}/tmp"
umount "${output_dir}/dev"
umount "${output_dir}/proc"
