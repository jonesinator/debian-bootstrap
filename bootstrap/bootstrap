#!/bin/sh -eu

################################################################################
# Configuration                                                                #
################################################################################

# Mirror configuration.
: "${mirror:="http://deb.debian.org/debian"}"
: "${distribution:="bookworm"}"
: "${architecture:="amd64"}"
: "${key_url:="https://ftp-master.debian.org/keys"}"
: "${keys:="archive-key-11.asc archive-key-12.asc release-12.asc"}"
: "${keyserver:="hkps://keyserver.ubuntu.com"}"
: "${fingerprints:="1F89983E0081FDE018F3CC9673A4F27B8DD47936"
                   "B8B80B5B623EAB6AD8775C45B7C5D7D6350947F8"
                   "4D64FEC119C2029067D6E791F8D2585B8783D481"}"

# Bootstrap configuration.
: "${variant="standard"}"

# Local directory configuration.
: "${cache_dir:="./cache"}"
: "${temp_dir:="./tmp"}"
: "${output_dir:="./debian"}"

# Validate the distribution.
case "${distribution}" in
    bookworm|trixie) ;;
    *) echo "Invalid distribution ${variant}." && exit 1 ;;
esac

# Validate the variant.
case "${variant}" in
    standard|buildd|minbase) ;;
    *) echo "Invalid variant ${variant}." && exit 1 ;;
esac

################################################################################
# Functions                                                                    #
################################################################################

cache_path() {
    url__="${1}"
    cache_dir__="${2}"

    protocol__="$(echo "${url__}" | cut -f1 -d:)"
    host__="$(echo "${url__}" | sed 's|^https\?://||' | cut -d/ -f1)"
    file_path__="$(echo "${url__}" | sed 's|^https\?://||' | cut -d/ -f2-)"
    echo "${cache_dir__}/${protocol__}/${host__}/${file_path__}"
}

download() {
    url_="${1}"
    cache_dir_="${2}"

    wget -nv --tries 5 --waitretry 30 \
        -nc "${url_}" -P "$(dirname "$(cache_path "${url_}" "${cache_dir_}")")"
    cache_path "${url_}" "${cache_dir_}"
}

# Takes two lists as arguments, and removes any line in the second from the
# first. The result is output on stdout.
remove_from() {
    printf "%s\n%s\n%s" "${1}" "${2}" "${2}" | sort | uniq -u
}

# Takes two lists of package infos as arguments and outputs the combined list to
# stdout.
concat_packages() {
    printf "%s\n\n%s" "${1}" "${2}"
}

# Takes a list of package names on stdin, escapes them for use in regexes, and
# outputs them on stdout.
escape_package_names() {
    sed 's|\+|\\+|g' | sed 's|\.|\\.|g'
}

# Takes a list of package infos on stdin and outputs just the value of the field
# across all of the package infos in the list on stdout
package_field() {
    sed -n "s/^${1}: //p"
}

# Runs a command in the target root filesystem. The first argument is the
# location of the root filesystem. The remaining arguments are the command to
# run.
in_target() {
    env -i PATH="/usr/bin:/usr/sbin" chroot "${@}"
}

# Prints the contents of the "Release" file of the package archive after
# validating its signatures. The contents of the signed "InRelease" file and the
# public keys used to validate it are cached in the cache directory.
get_release() {
    mirror_="${1}"
    distribution_="${2}"
    key_url_="${3}"
    keys_="${4}"
    fingerprints_="${5}"
    keyserver_="${6}"
    cache_dir_="${7}"
    temp_path_="${8}"

    # Download the signing keys and the InRelease file, then validate the
    # InRelease file. Note the "gpg" command below has "--output -" which sends
    # the file to stdout.
    rm -rf "${temp_path_}"
    mkdir -m 0700 "${temp_path_}"
    for key_ in ${keys_}; do
        key_path_="$(download "${key_url_}/${key_}" "${cache_dir_}")"
        gpg --homedir "${temp_path_}" --import "${key_path_}"
    done
    in_release_path_="$(download \
        "${mirror_}/dists/${distribution_}/InRelease" "${cache_dir_}")"
    gpg --homedir "${temp_path_}" --output - --verify "${in_release_path_}"
    rm -rf "${temp_path_}"

    # Validate the InRelease file using the fingerprints and a keyserver, if
    # requested.
    if [ "${keyserver_}" != "none" ]; then
        mkdir -m 0700 "${temp_path_}"
        for fingerprint_ in ${fingerprints_}; do
            gpg --homedir "${temp_path_}" \
                --keyserver "${keyserver_}" \
                --recv-keys "${fingerprint_}"
        done
        gpg --homedir "${temp_path_}" --verify "${in_release_path_}"
        rm -rf "${temp_path_}"
    fi
}

# Prints the contents a file described by the "Release" file of the package
# archive by path. Assumes gzip compression. The file is also cached in the
# cache directory.
get_release_file() {
    mirror_="${1}"
    distribution_="${2}"
    release_="${3}"
    path_="${4}"
    cache_dir_="${5}"
    temp_path_="${6}"

    # Parse the files from the release in the format " {sha256} {size} {path}".
    # The first sed starts capturing after the line with "SHA256:", and the
    # second sed stops capturing on the first line that does not stop with a
    # space.
    files_="$(echo "${release_}" | sed '1,/^SHA256:$/ d' | sed '/^[^ ]/Q')"

    # Extract the hash for the gzip compressed file, and use it to construct the
    # URL to download the file. Then download the file and check the hash.
    packed_hash_="$(echo "${files_}" | grep "${path_}.gz$" | awk '{print $1}')"
    hash_path_="$(dirname "${path_}")/by-hash/SHA256/${packed_hash_}"
    url_="${mirror_}/dists/${distribution_}/${hash_path_}"
    file_="$(download "${url_}" "${cache_dir_}")"
    echo "${packed_hash_}  ${file_}" | sha256sum -c --status

    # Decompress the downloaded file and check its hash as well.
    rm -rf "${temp_path_}"
    gunzip --stdout "${file_}" > "${temp_path_}"
    unpacked_hash_="$(echo "${files_}" | grep "${path_}$" | awk '{print $1}')"
    echo "${unpacked_hash_}  ${temp_path_}" | sha256sum -c --status
    cat "${temp_path_}"
    rm "${temp_path_}"
}

# Takes a set of package infos and filters them with awk and prints the results.
# First the "field" of the package info is matched, e.g. "^Package$" or
# "^(Filename|SHA256)$". Then the "value" at that field (not including the field
# name) is matched using a different regex.
#
# For example:
#     filter_packages "${packages}" "^Package$" "^(alpha|beta|gamma)$"
# Would print just the package infos for packages exactly named "alpha", "beta",
# or "gamma".
#
# This is rather slow, and fairly fragile since it is awk in sh, but it is
# fairly powerful and is really the only record-level filtering function that we
# need. This is the only non-trivial use of awk, so it would be good to replace
# with some combination of sh/grep/sed to reduce the tool count.
filter_packages() {
    filter_packages_="${1}"
    filter_field_="${2}"
    filter_value_="${3}"

    echo "${filter_packages_}" | awk -v RS= -v FS='\n' "{
        for (i = 1; i <= NF; ++i) {
            if (\$i ~ /^${filter_field_}: / &&
                    substr(\$i, index(\$i, \" \") + 1) ~ /${filter_value_}/) {
                print \$0 \"\\n\"
                break
            }
        }
    }"
}

# Take a list of package infos that may have duplicates and print just the
# unique ones. The order of the incoming list is not preserved.
unique_packages() {
    packages_="${1}"

    package_names_="$(echo "${1}" |
        package_field "Package" | sort | uniq | escape_package_names)"
    for package_ in ${package_names_}; do
        filter_packages "${packages_}" "Package" "^${package_}$" | sed '/^$/q'
    done
}

# Takes a list of package infos and regex for package names, and gets the path
# to the .deb file for the package. The paths can only be used inside the target
# chroot.
deb_for_packages() {
    filter_packages "${1}" "Package" "^${2}$" |
        package_field "Filename" | sed "s|^|$(cache_path "${3}/" "/cache")/|"
}

# Gets the non-recursive list of dependencies for a set of packages. Prints just
# new packages that the given set of packages depends on. If this function
# succeeds and prints nothing, then you'll know you have all dependencies and
# the set of packages is self-consistent. See "get_package_dependencies" for the
# function that iterates over this function.
get_direct_package_dependencies() {
    direct_all_packages_="${1}"
    direct_install_packages_="${2}"
    direct_exclude_packages_="${3}"

    direct_names_="$(echo "${direct_install_packages_}" |
        package_field "Package")"
    direct_depends_="$(echo "${direct_install_packages_}" |
        package_field '\(Pre-\)\?Depends')"

    # Gather the direct dependencies of the install package set. The sed
    # operations convert comma-separated to line-separated, remove version
    # constraints, select the first alternative for everything, and ignore any
    # architecture specification.
    direct_depends_all_="$(echo "${direct_depends_}" | sed 's/, /\n/g' |
        sed 's/ (.*)//g' | sed 's/ |.*$//' | sed 's/:.*//' | sort | uniq)"
    direct_dep_names_="$(remove_from \
        "${direct_depends_all_}" "${direct_names_}" | escape_package_names)"

    # Construct a regex like "(^| )(dep1|dep2|dep3)([ ,]|$)" which will match
    # any of the dependency names in either the "Package" or "Provides" fields.
    direct_inner_regex_="$(printf "%s" "${direct_dep_names_}" | tr '\n' '|')"
    direct_regex_="$(printf "(^| )(%s)([ ,]|$)" "${direct_inner_regex_}")"

    # Get all packages that might satisfy the dependencies, i.e. get the package
    # info for any package whose "Package" or "Provides" field contains any of
    # the dependency names.
    direct_dep_candidates_="$(filter_packages \
        "${direct_all_packages_}" "(Package|Provides)" "${direct_regex_}")"

    # For each dependency select a package from all of the candidates.
    direct_added_=""
    for direct_dep_ in ${direct_dep_names_}; do
        # If this dependency is excluded, nothing to do.
        if echo "${direct_dep_}" | grep -qE "${direct_exclude_packages_}"; then
            continue
        fi

        # If this dependency is already installed, there's nothing to do.
        echo "${direct_added_}" | grep --quiet "${direct_dep_}" && continue

        # Look for a package with the exact name. If one is found, use it.
        direct_package_="$(filter_packages \
            "${direct_dep_candidates_}" "Package" "^${direct_dep_}$")"
        if [ -n "${direct_package_}" ]; then
            printf "%s\n\n" "${direct_package_}"
            direct_added_="${direct_dep_} ${direct_added_}"
            continue
        fi

        # Look for a virtual package that has already been selected.
        direct_candidates_="$(filter_packages "${direct_dep_candidates_}" \
            "Provides" "(^| )${direct_dep_}([ ,]|$)")"
        direct_candidate_names_="$(echo "${direct_candidates_}" |
            sed -n 's/^Package: //p' | escape_package_names)"
        direct_found_=0
        for direct_candidate_ in ${direct_candidate_names_}; do
            if printf "%s\n%s" "${direct_names_}" "${direct_added_}" |
                    grep --quiet "${direct_candidate_}"; then
                direct_found_=1
                break
            fi
        done
        if [ "${direct_found_}" -eq 1 ]; then continue; fi

        # If there's exactly one candidate, then the choice is clear.
        if [ "$(echo "${direct_candidate_names_}" | wc -l)" -eq 1 ]; then
            printf "%s\n\n" "${direct_candidates_}"
            direct_added_="${direct_candidate_names_} ${direct_added_}"
            continue
        fi

        # Fortunately, we don't need anything more refined than the above at the
        # moment...
        echo "Unable to find package for ${direct_dep_}" 1>&2
        exit 1
    done
}

# Gets the full list of dependencies for a set of packages, including
# dependencies of dependencies recursively. Prints both the original list of
# packages that came in, and all of the dependencies of those packages.
get_package_dependencies() {
    all_="${1}"
    packages_="${2}"
    exclude_="${3}"

    while deps_="$(get_direct_package_dependencies \
            "${all_}" "${packages_}" "${exclude_}")" && [ -n "${deps_}" ]; do
        packages_="$(concat_packages "${packages_}" "${deps_}")"
    done
    echo "${packages_}"
}

# Download the .deb archives for a set of packages to the cache directory.
get_packages() {
    mirror_="${1}"
    packages_="${2}"
    cache_dir_="${3}"

    package_names_="$(echo "${packages_}" |
        package_field "Package" | escape_package_names)"
    for package_ in ${package_names_}; do
        package_info_="$(filter_packages \
            "${packages_}" "Package" "^${package_}$")"
        path_="$(echo "${package_info_}" | package_field "Filename")"
        sha256_="$(echo "${package_info_}" | package_field "SHA256")"
        file_="$(download "${mirror_}/${path_}" "${cache_dir_}")"
        echo "${sha256_}  ${file_}" | sha256sum -c --status
    done
}

# Unpacks 
unpack_packages() {
    mirror_="${1}"
    packages_="${2}"
    cache_dir_="${3}"
    output_="${4}"
    temp_dir_="${5}"

    output_dir_="$(pwd)/${output_}"
    rm -rf "${output_dir_:?}"/* "${temp_dir_}"
    mkdir -p "${output_dir_}" "${temp_dir_}"
    for file_ in $(echo "${packages_}" | sed -n 's/^Filename: //p'); do
      restore_="$(pwd)"
      unpack_dir_="${temp_dir_}/$(basename "${file_}")"
      full_path_="$(cache_path "${mirror_}/${file_}" "${cache_dir_}")"
      mkdir -p "${unpack_dir_}"
      cp "${full_path_}" "${unpack_dir_}/package.deb"
      cd "${unpack_dir_}"
      ar x package.deb
      rm package.deb

      mkdir control data
      tar xf control.tar.* -C control
      tar xf data.tar.* -C data
      tar xf data.tar.* -C "${output_dir_}"
      rm control.tar.* data.tar.*
      cd data
      md5sum -c --status ../control/md5sums

      cd "${restore_}"
    done
}

################################################################################
# Main Script                                                                  #
################################################################################

# Get the package metadata.
release="$(get_release \
    "${mirror}" "${distribution}" "${key_url}" "${keys}" "${fingerprints}" \
    "${keyserver}" "${cache_dir}" "${temp_dir}")"
all_packages="$(get_release_file \
    "${mirror}" "${distribution}" "${release}" \
    "main/binary-${architecture}/Packages" "${cache_dir}" "${temp_dir}")"

# Determine the "required" packages.
exclude_packages="usrmerge"
additional_packages="usr-is-merged"
if [ "${distribution}" = "trixie" ] && [ "${variant}" = "buildd" ]; then
    additional_packages="mawk|${additional_packages}"
    initial_required_packages="$(filter_packages "${all_packages}" "Essential" "^yes$")"
else
    initial_required_packages="$(filter_packages "${all_packages}" "Priority" "^required$")"
fi

additional_required_packages="$(filter_packages \
    "${all_packages}" "Package" "^(${additional_packages})$")"
direct_required_packages="$(concat_packages \
    "${initial_required_packages}" "${additional_required_packages}")"
required_packages="$(get_package_dependencies "${all_packages}" \
    "${direct_required_packages}" "^(${exclude_packages})$")"

# Determine the "base" packages.
case "${variant}" in
    standard) direct_base_packages="$(filter_packages "${all_packages}" "Priority" "^important$")"            ;;
    buildd)   direct_base_packages="$(filter_packages "${all_packages}" "Package" "^(apt|build-essential)$")" ;;
    minbase)  direct_base_packages="$(filter_packages "${all_packages}" "Package" "^apt$")"                   ;;
esac
case "${mirror}" in
    https://*) extra_base_packages="$(filter_packages "${all_packages}" "Package" "^ca-certificates$")" ;;
    *) extra_base_packages="" ;;
esac
direct_base_packages="$(concat_packages "${direct_base_packages}" "${extra_base_packages}")"
base_packages="$(get_package_dependencies \
    "${all_packages}" "${direct_base_packages}" "^(${exclude_packages})$")"

# Download all of the packages in the combined "required" and "base" set.
install_packages_with_duplicates="$(concat_packages \
    "${required_packages}" "${base_packages}")"
install_packages="$(unique_packages "${install_packages_with_duplicates}")"
get_packages "${mirror}" "${install_packages}" "${cache_dir}"

# Unpack just the "required" packages.
rm -rf "${temp_dir}"
mkdir "${temp_dir}"
unpack_packages "${mirror}" "${required_packages}" \
    "${cache_dir}" "${output_dir}" "${temp_dir}"
rm -rf "${temp_dir}"

# Perform the usr directory merge.
restore="$(pwd)"
cd "${output_dir}"
for dir in bin lib sbin; do
    [ -L "${dir}" ] && continue
    cp -r "${dir}"/* "usr/${dir}/"
    rm -rf "${dir}"
    ln -s "usr/${dir}" "${dir}"
done
if [ -e lib64 ] && [ ! -L lib64 ]; then
    mv lib64 usr/lib64
    ln -s usr/lib64 lib64
fi
cd "${restore}"

# Ensure a copy of all of the downloaded files is available within the target
# filesystem.
cp -r cache "${output_dir}/cache"

# Need to mount proc otherwise some files are not created (presumably by
# systemd-tmpfiles?)
# TODO Move this as low as possible, and unmount it right after.
mkdir -p "${output_dir}/proc"
mount -t proc proc "${output_dir}/proc"

# Install some early required packages. Put a temporary awk symlink in place
# until we can install mawk. This ordering is taken from debootstrap, but no
# reasoning is given for it.
ln -sf mawk "${output_dir}/usr/bin/awk"
for package in base-passwd base-files dpkg libc6; do
    in_target "${output_dir}" dpkg --force-depends --install \
        "$(deb_for_packages "${required_packages}" "${package}" "${mirror}")"
done
rm "${output_dir}/usr/bin/awk"
for package in mawk debconf; do
    in_target "${output_dir}" dpkg --force-depends --install \
        "$(deb_for_packages "${required_packages}" "${package}" "${mirror}")"
done

# Now we can unpack then configure all required packages. The configure requires
# the timezone to be set or there's an interactive prompt installing tzdata.
# Assume UTC, it is the one true timezone.
# shellcheck disable=SC2046
in_target "${output_dir}" dpkg --force-depends --unpack \
    $(deb_for_packages "${required_packages}" ".*" "${mirror}")
ln -sf /usr/share/zoneinfo/UTC "${output_dir}/etc/localtime"
in_target "${output_dir}" \
    dpkg --configure --pending --force-configure-any --force-depends

# Configure some additional dpkg/apt files.
echo "deb ${mirror} ${distribution} main" > "${output_dir}/etc/apt/sources.list"
echo "apt apt" > "${output_dir}/var/lib/dpkg/cmethopt"
chmod 644 "${output_dir}/var/lib/dpkg/cmethopt"

echo "${install_packages}" > "${output_dir}/var/lib/dpkg/available"
echo "${install_packages}" | package_field "Package" | sed 's/$/ install/' |
    in_target "${output_dir}" dpkg --set-selections

# Get the list of base packages that we haven't already installed.
required_package_names="$(echo "${required_packages}" |
    package_field "Package")"
remaining_base_package_names="$(echo "${base_packages}" |
    package_field "Package")"
remaining_base_package_names="$(remove_from \
    "${remaining_base_package_names}" "${required_package_names}")"

# Install predeps that haven't yet been satisfied. Those predeps have
# dependencies of their own that may not have been installed, and we need to
# determine those manually. Unfortunately, we need to be careful not to call
# dpkg "--install" on a package that has not already been installed, so we take
# pains to track that. The install actually succeeds, but the only side-effect
# is that the priorties of some packages mysteriously change. I'm not sure why
# that is, but it causes a difference in the /var/lib/dpkg/status file from
# debootstrap, so do the tracking here so we get the same binary result even
# though a simpler algorithm is probably the same logical result here.
installed_predeps=""
while predep="$(in_target "${output_dir}" dpkg --predep-package)"; do
    predep_dependencies="$(get_package_dependencies \
        "${install_packages}" "${predep}" "^(${exclude_packages})$")"
    predep_package_names_unfiltered="$(echo "${predep_dependencies}" |
        package_field "Package")"
    predep_package_names_without_required="$(remove_from \
        "${predep_package_names_unfiltered}" "${required_package_names}")"
    predep_package_names="$(remove_from \
        "${predep_package_names_without_required}" "${installed_predeps}")"
    predep_package_names_regex="^($(echo "${predep_package_names}" |
        tr '\n' '|' | sed 's/^|//' | sed 's/|$//'))$"
    predep_archives="$(deb_for_packages \
        "${install_packages}" "${predep_package_names_regex}" "${mirror}")"
    # shellcheck disable=SC2086
    in_target "${output_dir}" dpkg --force-overwrite --force-confold \
        --skip-same-version --install ${predep_archives}
    remaining_base_package_names="$(remove_from \
        "${remaining_base_package_names}" "${predep_package_names}")"
    installed_predeps="$(printf "%s\n%s" "${installed_predeps}" \
        "${predep_package_names}")"
done

# Now we can unpack and configure the rest of the "base" package set.
if [ -n "${remaining_base_package_names}" ]; then
    remaining_regex="^($(echo "${remaining_base_package_names}" | escape_package_names | tr '\n' '|'))$"
    remaining_base_packages="$(filter_packages \
        "${base_packages}" "Package" "${remaining_regex}")"
    remaining_base_package_archives="$(deb_for_packages \
        "${remaining_base_packages}" ".*" "${mirror}")"
    # shellcheck disable=SC2086
    in_target "${output_dir}" dpkg --force-overwrite --force-confold \
        --skip-same-version --unpack ${remaining_base_package_archives}
fi

in_target "${output_dir}" \
    dpkg --force-confold --skip-same-version --configure -a

# Clean up.
rm -rf "${output_dir}/cache"
umount "${output_dir}/proc"

################################################################################
# Reproducibility Cleanup                                                      #
################################################################################

# The contents of dev are mostly decided by the Linux kernel, and can easily
# be recreated when needed.
rm -rfv "${output_dir:?}"/dev/*

# May appear sometimes.
rm -rfv "${output_dir}"/etc/apparmor.d/local/sbin.dhclient

# Both scripts generate this random ID. Remove it since it can easily be
# regenerated later and is a source of non-reproducibility.
rm -rfv "${output_dir}"/etc/machine-id

# According to the Linux FHS, the /run directory is to be cleared each boot, so
# it should be fine to remove its contents.
# https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch03s15.html
rm -rfv "${output_dir}"/run/*

# According to the Linux FHS, the application must be able to regenerate or
# restore any data in the /var/cache directory, so it should be fine to
# remove its contents.
rm -rfv "${output_dir}"/var/cache/*

# According to the Linux FHS, the application must be able to regenerate or
# restore any data in the /var/cache directory, so it should be fine to
# remove its contents.
rm -rfv "${output_dir}"/var/lib/apt/lists/*

# This is a file used by dpkg to know what packages are available. The local
# ./bootstrap script and debootstrap both generate basically the same file,
# but the ordering is different. They could be made the same with some
# effort, but this file is not actually essential and can be regenerated
# from the package list, so it should be good to remove here.
rm -rfv "${output_dir}"/var/lib/dpkg/available

# This is a file that can be used by dpkg for backup purposes, but is
# generally not needed, especially on a just-bootstrapped system.
rm -rfv "${output_dir}"/var/lib/dpkg/status-old

# These are log files that have timestamps which are a source of
# non-reproducibility.
rm -rfv "${output_dir}"/var/log/dpkg.log
rm -rfv "${output_dir}"/var/log/alternatives.log

# Ensure some timestamps are updated.
touch "${output_dir}"/proc
touch "${output_dir}"/usr/lib/mime
touch "${output_dir}"/usr/lib/terminfo
touch "${output_dir}"/usr/libexec
touch "${output_dir}"/usr/share
touch "${output_dir}"/usr/share/bash-completion
touch "${output_dir}"/usr/share/doc
touch "${output_dir}"/usr/share/lintian
touch "${output_dir}"/usr/share/locale
touch "${output_dir}"/usr/share/locale/*
touch "${output_dir}"/usr/share/man
touch "${output_dir}"/usr/share/man/*
touch "${output_dir}"/usr/share/perl5
touch "${output_dir}"/var/lib/apt
