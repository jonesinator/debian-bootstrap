#!/bin/sh
#
# Use "local" even though POSIX shell doesn't actually support it:
# shellcheck disable=SC3043
set -eu

################################################################################
# Configuration                                                                #
################################################################################

# Bootstrapping configuration.
: "${architecture:="amd64"}"
: "${distribution:="bookworm"}"
: "${variant="standard"}"

# Mirror configuration.
: "${mirror:="http://deb.debian.org/debian"}"
: "${key_url:="https://ftp-master.debian.org/keys"}"
: "${keys:="archive-key-11.asc archive-key-12.asc release-12.asc"}"
: "${keyserver:="hkps://keyserver.ubuntu.com"}"
: "${fingerprints:="1F89983E0081FDE018F3CC9673A4F27B8DD47936"
                   "B8B80B5B623EAB6AD8775C45B7C5D7D6350947F8"
                   "4D64FEC119C2029067D6E791F8D2585B8783D481"}"

# Local directory configuration.
: "${cache_dir:="./cache"}"
: "${temp_dir:="./tmp"}"
: "${output_dir:="./debian"}"

################################################################################
# Functions                                                                    #
################################################################################

# Takes two lists as arguments, prints the first list to stdout except for any
# lines that are also in the second list. Order is not preserved.
remove_from() {
	local list="${1}"
	local remove="${2}"

	printf "%s\n%s\n%s" "${list}" "${remove}" "${remove}" | sort | uniq -u
}

# Escapes special characters in package names for use in regular expressions.
escape_names() {
	sed -e 's/[+.]/\\&/g'
}

# Runs a command in the target root filesystem. The first argument is the
# location of the root filesystem. The remaining arguments are the command to
# run.
in_target() {
	env -i PATH="/usr/bin:/usr/sbin" chroot "${@}"
}

# Print where a file should be stored in the cache based on its URL.
cache_path() {
	local url="${1}"       # The URL of the file path to compute.
	local cache_dir="${2}" # The directory where cached files are sored.
	local protocol host file_path

	protocol="$(echo "${url}" | cut -f1 -d:)"
	host="$(echo "${url}" | sed 's|^https\?://||' | cut -d/ -f1)"
	file_path="$(echo "${url}" | sed 's|^https\?://||' | cut -d/ -f2-)"
	echo "${cache_dir}/${protocol}/${host}/${file_path}"
}

# Download a file into the cache by its URL, and print the path to it. If the
# file already exists it is not downloaded again.
download() {
	local url="${1}"       # The URL of the file to download.
	local cache_dir="${2}" # The directory where cached files are stored.
	local dest

	dest="$(cache_path "${url}" "${cache_dir}")"
	wget -nv -nc --tries 5 --waitretry 30 -P "$(dirname "${dest}")" "${url}"
	echo "${dest}"
}

# Prints the contents of the "Release" file of the package archive after
# validating its signatures. The contents of the signed "InRelease" file and the
# public keys used to validate it are cached in the cache directory.
get_release() {
	local mirror="${1}"       # Base URL for the Debian archive.
	local distribution="${2}" # Name of the Debian distribution being used.
	local key_url="${3}"      # Base URL to download key files.
	local keys="${4}"         # Name of key files to download.
	local fingerprints="${5}" # Fingerprints of public keys to validate.
	local keyserver="${6}"    # Keyserver to use to validate public keys.
	                          # Use "none" to disable.
	local cache_dir="${7}"    # Directory used to store downloaded files.
	local temp_dir="${8}"     # Path for temporary gpg usage.
	local key key_path file_path fingerprint url

	# Download the keys and the file, then validate the file using the keys.
	rm -rf "${temp_dir}"
	mkdir -m 0700 "${temp_dir}"
	for key in ${keys}; do
		key_path="$(download "${key_url}/${key}" "${cache_dir}")"
		gpg --homedir "${temp_dir}" --import "${key_path}"
	done
	url="${mirror}/dists/${distribution}/InRelease"
	file_path="$(download "${url}" "${cache_dir}")"
	gpg --homedir "${temp_dir}" --verify "${file_path}"
	rm -rf "${temp_dir}"

	# Validate the InRelease file using the fingerprints and a keyserver.
	if [ "${keyserver}" != "none" ]; then
		mkdir -m 0700 "${temp_dir}"
        	for fingerprint in ${fingerprints}; do
			gpg --homedir "${temp_dir}" --keyserver "${keyserver}" \
				--recv-keys "${fingerprint}"
		done
		gpg --homedir "${temp_dir}" --verify "${file_path}"
		rm -rf "${temp_dir}"
	fi

	echo "${file_path}"
}

# Prints the verified contents a file described by the "Release" file of the
# package archive by path.
get_release_file() {
	local mirror="${1}"       # Base URL for the Debian archive.
	local distribution="${2}" # Name of the Debian distribution being used.
	local release="${3}"      # Path to the InRelease file.
	local path="${4}"         # Path of the file to download.
	local cache_dir="${5}"    # Directory used to store downloaded files.
	local output_path="${6}"  # Location to put the final file contents.
	local files compression name sum by_hash download_path url file

	# Parse the files from the release in the format " {sha256} {size}
	# {path}". The first sed starts capturing after the line with "SHA256:",
	# and the second sed stops capturing on the first line that does _not_
	# start with a space.
	files="$(sed '1,/^SHA256:$/ d' "${release}" | sed '/^[^ ]/Q')"

	# Look for the compressed file.
	for compression in "xz" "gz"; do
		name="${path}.${compression}"
		sum="$(echo "${files}" | grep " ${name}$" | cut -d' ' -f2)"
		if [ -n "${sum}" ]; then break; fi
	done
	[ -z "${sum}" ] && return 1

	# Prefer getting files by their hash if possible.
	by_hash="$(sed -n 's/Acquire-By-Hash: //p' "${release}" || true)"
	if [ "${by_hash}" = "yes" ]; then
		download_path="$(dirname "${path}")/by-hash/SHA256/${sum}"
	else
		download_path="${name}"
	fi

	# Get the compressed file and check its hash.
	url="${mirror}/dists/${distribution}/${download_path}"
	file="$(download "${url}" "${cache_dir}")"
	echo "${sum}  ${file}" | sha256sum -c --status

	# Decompress the downloaded file and check its hash as well.
	case "${compression}" in
		gz)
			cp -p "${file}" "${output_path}.gz"
			gzip --decompress "${output_path}.gz"
			;;
		xz)
			cp -p "${file}" "${output_path}.xz"
			xz --decompress "${output_path}.xz"
			;;
	esac
	sum="$(echo "${files}" | grep "${path}$" | cut -d' ' -f2)"
	echo "${sum}  ${output_path}" | sha256sum -c --status
}

# Takes a set of package infos and filters them with awk and prints the results.
# First the "field" of the package info is matched, e.g. "^Package$" or
# "^(Filename|SHA256)$". Then the "value" at that field (not including the field
# name) is matched using a different regex.
#
# For example:
#     filter_packages "${packages}" "Package" "^(alpha|beta|gamma)$"
# Would print just the package infos for packages exactly named "alpha", "beta",
# or "gamma".
#
# This is rather slow, and fairly fragile since it is awk in sh, but it is
# fairly powerful and is really the only record-level filtering function that we
# need. This is the only non-trivial use of awk, so it would be good to replace
# with some combination of sh/grep/sed to reduce the tool count.
filter_packages() {
	local packages="${1}" # The list of package infos to filter.
	local field="${2}"    # The regex to use for the field name.
	local val="${3}"      # The regex to use for the field value.

	echo "${packages}" | awk -v RS= -v FS='\n' "{
		for (i = 1; i <= NF; ++i) {
			if (\$i ~ /^${field}: / &&
				substr(\$i, index(\$i, \" \") + 1) ~ /${val}/) {
				print \$0 \"\\n\"
				break
			}
		}
	}"
}

# Take a list of package infos on stdin that may have duplicates and print just
# the unique ones. The order of the incoming list is not preserved.
unique_packages() {
	local packages="${1}" # The list of package infos to filter.
	local names pkg

	names="$(echo "${packages}" | sed -n "s/^Package: //p" |
		sort | uniq | escape_names)"
	for pkg in ${names}; do
		filter_packages "${packages}" "Package" "^${pkg}$" | sed '/^$/q'
	done
}

# Takes a list of package infos and regex for package names, and gets the path
# to the .deb file for the package. The paths can only be used inside the target
# chroot.
deb_for_packages() {
	filter_packages "${1}" "Package" "^${2}$" | sed -n "s/^Filename: //p" |
		sed "s|^|$(cache_path "${3}/" "/cache")/|"
}

# Gets the non-recursive list of dependencies for a set of packages. Prints just
# new packages that the given set of packages depends on. If this function
# succeeds and prints nothing, then you'll know you have all dependencies and
# the set of packages is self-consistent. See "get_dependencies" for the
# function that iterates over this function.
get_direct_dependencies() {
	local all="${1}"     # The set of all package infos to pull from.
	local install="${2}" # The set of package infos to get dependencies for.
	local exclude="${3}" # A regex list of package names to exclude.
	local names depends depends_all dep_names inner regex candidates added
	local dep pkg virtual_candidates virtual_candidate virtual_names found

	names="$(echo "${install}" | sed -n "s/^Package: //p")"
	depends="$(echo "${install}" | sed -n "s/^\(Pre-\)\?Depends: //p")"

	# Gather the direct dependencies of the install package set. The sed
	# operations convert comma-separated to line-separated, remove version
	# constraints, select the first alternative for everything, and ignore
	# any architecture specification.
	depends_all="$(echo "${depends}" | sed 's/, /\n/g' | sed 's/ (.*)//g' |
		sed 's/ |.*$//' | sed 's/:.*//' | sort | uniq)"
	dep_names="$(remove_from "${depends_all}" "${names}" | escape_names)"

	# Construct a regex like "(^| )(dep1|dep2|dep3)([ ,]|$)" which will
	# match any of the dependency names in either the "Package" or
	# "Provides" fields.
	inner="$(printf "%s" "${dep_names}" | tr '\n' '|')"
	regex="$(printf "(^| )(%s)([ ,]|$)" "${inner}")"

	# Get all packages that might satisfy the dependencies, i.e. get the
	# package info for any package whose "Package" or "Provides" field
	# contains any of the dependency names.
	candidates="$(filter_packages "${all}" "(Package|Provides)" "${regex}")"

	# For each dependency select a package from all of the candidates.
	added=""
	for dep in ${dep_names}; do
		# If this dependency is excluded or already installed, done!
		echo "${dep}" | grep -qE "${exclude}" && continue
		echo "${added}" | grep -q "${dep}" && continue

		# Use a package with the exact name, if one exists.
		pkg="$(filter_packages "${candidates}" "Package" "^${dep}$")"
		if [ -n "${pkg}" ]; then
			printf "%s\n\n" "${pkg}"
			added="${dep} ${added}"
			continue
		fi

		# Look for a virtual package that has already been selected.
		virtual_candidates="$(filter_packages "${candidates}" \
			"Provides" "(^| )${dep}([ ,]|$)")"
		virtual_names="$(echo "${virtual_candidates}" |
			sed -n 's/^Package: //p' | escape_names)"
		found=0
		for virtual_candidate in ${virtual_names}; do
			if printf "%s\n%s" "${names}" "${added}" |
			    grep --quiet "${virtual_candidate}"; then
				found=1
				break
			fi
		done
		[ "${found}" -eq 1 ] && continue

		# If there's exactly one candidate, then the choice is clear.
		if [ "$(echo "${virtual_names}" | wc -l)" -eq 1 ]; then
			printf "%s\n\n" "${virtual_candidates}"
			added="${virtual_names} ${added}"
			continue
		fi

		# Fortunately, we don't need anything more refined than the
		# above at the moment...
		echo "Unable to find package for ${dep}" 1>&2
		exit 1
	done
}

# Gets the full list of dependencies for a set of packages, including
# dependencies of dependencies recursively. Prints both the original list of
# packages that came in, and all of the dependencies of those packages.
get_dependencies() {
	local all="${1}"     # The set of all package infos to pull from.
	local install="${2}" # The set of package infos to get dependencies for.
	local exclude="${3}" # A regex list of package names to exclude.

	while deps="$(get_direct_dependencies \
		"${all}" "${install}" "${exclude}")" && [ -n "${deps}" ]; do
		install="$(printf "%s\n\n%s" "${install}" "${deps}")"
	done
	echo "${install}"
}

# Download the .deb archives for a set of packages to the cache directory.
get_packages() {
	local mirror="${1}"    # Base URL for the Debian archive.
	local packages="${2}"  # The package infos of the packages to download.
	local cache_dir="${3}" # Directory where downloaded files are stored.
	local names package info path sha256 file

	names="$(echo "${packages}" | sed -n "s/^Package: //p" | escape_names)"
	for package in ${names}; do
		info="$(filter_packages "${packages}" "Package" "^${package}$")"
		path="$(echo "${info}" | sed -n "s/^Filename: //p")"
		sha256="$(echo "${info}" | sed -n "s/^SHA256: //p")"
		file="$(download "${mirror}/${path}" "${cache_dir}")"
		echo "${sha256}  ${file}" | sha256sum -c --status
	done
}

# Unpacks a list of (already-downloaded) packages.
unpack_packages() {
	local mirror="${1}"    # Base URL for the Debian archive.
	local packages="${2}"  # Package infos of the packages to unpack.
	local cache_dir="${3}" # Directory where downloaded files are stored.
	local output="${4}"    # Directory where packages are unpacked.
	local temp_dir="${5}"  # Temporary directory for scratch space.
	local output_dir file restore unpack_dir full_path

	output_dir="$(pwd)/${output}"
	rm -rf "${output_dir:?}"/* "${temp_dir}"
	mkdir -p "${output_dir}" "${temp_dir}"
	for file in $(echo "${packages}" | sed -n 's/^Filename: //p'); do
		restore="$(pwd)"
		unpack_dir="${temp_dir}/$(basename "${file}")"
		full_path="$(cache_path "${mirror}/${file}" "${cache_dir}")"
		mkdir -p "${unpack_dir}"
		cp "${full_path}" "${unpack_dir}/package.deb"
		cd "${unpack_dir}"
		ar x package.deb
		rm package.deb

		mkdir control data
		tar xf control.tar.* -C control
		tar xf data.tar.* -C data
		tar xf data.tar.* -C "${output_dir}"
		rm control.tar.* data.tar.*
		cd data
		md5sum -c --status ../control/md5sums

		cd "${restore}"
	done
}

################################################################################
# Main Script                                                                  #
################################################################################

# Get the package metadata.
release="$(get_release "${mirror}" "${distribution}" "${key_url}" "${keys}" \
	"${fingerprints}" "${keyserver}" "${cache_dir}" "${temp_dir}")"
get_release_file "${mirror}" "${distribution}" "${release}" \
	"main/binary-${architecture}/Packages" "${cache_dir}" "${temp_dir}"
all_packages="$(cat "${temp_dir}")"

# Determine the "required" packages.
exclude_packages="usrmerge"
additional_packages="usr-is-merged"
if [ "${distribution}" = "trixie" ] && [ "${variant}" = "buildd" ]; then
	additional_packages="mawk|${additional_packages}"
	initial_required_packages="$(filter_packages \
		"${all_packages}" "Essential" "^yes$")"
else
	initial_required_packages="$(filter_packages \
		"${all_packages}" "Priority" "^required$")"
fi
additional_required_packages="$(filter_packages \
	"${all_packages}" "Package" "^(${additional_packages})$")"
direct_required_packages="$(printf "%s\n\n%s" \
	"${initial_required_packages}" "${additional_required_packages}")"
required_packages="$(get_dependencies "${all_packages}" \
	"${direct_required_packages}" "^(${exclude_packages})$")"

# Determine the "base" packages.
case "${variant}" in
	standard)
		direct_base_packages="$(filter_packages \
			"${all_packages}" "Priority" "^important$")"
		;;
	buildd)
		direct_base_packages="$(filter_packages \
			"${all_packages}" "Package" "^(apt|build-essential)$")"
		;;
	minbase)
		direct_base_packages="$(filter_packages \
			"${all_packages}" "Package" "^apt$")"
		;;
esac
case "${mirror}" in
	https://*)
		extra_base_packages="$(filter_packages \
			"${all_packages}" "Package" "^ca-certificates$")"
		;;
	*)
		extra_base_packages=""
		;;
esac
direct_base_packages="$(printf "%s\n\n%s" \
	"${direct_base_packages}" "${extra_base_packages}")"
base_packages="$(get_dependencies \
	"${all_packages}" "${direct_base_packages}" "^(${exclude_packages})$")"

# Download all of the packages.
with_dups="$(printf "%s\n\n%s" "${required_packages}" "${base_packages}")"
install_packages="$(unique_packages "${with_dups}")"
get_packages "${mirror}" "${install_packages}" "${cache_dir}"

# Unpack all of the required packages. These constitute the initial filesystem.
unpack_packages "${mirror}" "${required_packages}" \
	"${cache_dir}" "${output_dir}" "${temp_dir}"

# Perform the usr directory merge (if it has not already been done).
restore="$(pwd)"
cd "${output_dir}"
for dir in bin lib sbin; do
	[ -L "${dir}" ] && continue
	cp -r "${dir}"/* "usr/${dir}/"
	rm -rf "${dir}"
	ln -s "usr/${dir}" "${dir}"
done
if [ -e lib64 ] && [ ! -L lib64 ]; then
	mv lib64 usr/lib64
	ln -s usr/lib64 lib64
fi
cd "${restore}"

# Ensure a copy of all of the downloaded files is available within the target
# filesystem.
cp -r cache "${output_dir}/cache"

mkdir -p "${output_dir}/proc"
mount -t proc proc "${output_dir}/proc"

# Install some early required packages. Put a temporary awk symlink in place
# until we can install mawk. This ordering is taken from debootstrap, but no
# reasoning is given for it.
ln -sf mawk "${output_dir}/usr/bin/awk"
for pkg in base-passwd base-files dpkg libc6; do
	deb="$(deb_for_packages "${required_packages}" "${pkg}" "${mirror}")"
	in_target "${output_dir}" dpkg --force-depends --install "${deb}"
done
rm "${output_dir}/usr/bin/awk"
for pkg in mawk debconf; do
	deb="$(deb_for_packages "${required_packages}" "${pkg}" "${mirror}")"
	in_target "${output_dir}" dpkg --force-depends --install "${deb}"
done

# Now we can unpack then configure all required packages. The configure requires
# the timezone to be set or there's an interactive prompt installing tzdata.
# Assume UTC, it is the one true timezone.
# shellcheck disable=SC2046
in_target "${output_dir}" dpkg --force-depends --unpack \
	$(deb_for_packages "${required_packages}" ".*" "${mirror}")
ln -sf /usr/share/zoneinfo/UTC "${output_dir}/etc/localtime"
in_target "${output_dir}" \
	dpkg --configure --pending --force-configure-any --force-depends

# Configure some additional dpkg/apt files.
echo "deb ${mirror} ${distribution} main" > "${output_dir}/etc/apt/sources.list"
echo "apt apt" > "${output_dir}/var/lib/dpkg/cmethopt"
chmod 644 "${output_dir}/var/lib/dpkg/cmethopt"

echo "${install_packages}" > "${output_dir}/var/lib/dpkg/available"
echo "${install_packages}" | sed -n "s/^Package: //p" | sed 's/$/ install/' |
	in_target "${output_dir}" dpkg --set-selections

# Get the list of base packages that we haven't already installed.
required_names="$(echo "${required_packages}" | sed -n "s/^Package: //p")"
remaining="$(echo "${base_packages}" | sed -n "s/^Package: //p")"
remaining="$(remove_from "${remaining}" "${required_names}")"

# Install predeps that haven't yet been satisfied. Those predeps have
# dependencies of their own that may not have been installed, and we need to
# determine those manually. Unfortunately, we need to be careful not to call
# dpkg "--install" on a package that has already been installed, so we take
# pains to track that. The install actually succeeds, but the only side-effect
# is that the priorties of some packages mysteriously change. I'm not sure why
# that is, but it causes a difference in the /var/lib/dpkg/status file from
# debootstrap, so do the tracking here so we get the same binary result even
# though a simpler algorithm is probably the same logical result here.
installed_predeps=""
while predep="$(in_target "${output_dir}" dpkg --predep-package)"; do
	deps="$(get_dependencies \
		"${install_packages}" "${predep}" "^(${exclude_packages})$")"
	names_all="$(echo "${deps}" | sed -n "s/^Package: //p")"
	names_minus_required="$(remove_from "${names_all}" "${required_names}")"
	names="$(remove_from "${names_minus_required}" "${installed_predeps}")"
	regex="^($(echo "${names}" | tr '\n' '|' |
		sed 's/^|//' | sed 's/|$//'))$"
	archives="$(deb_for_packages \
		"${install_packages}" "${regex}" "${mirror}")"
	# shellcheck disable=SC2086
	in_target "${output_dir}" dpkg --force-overwrite --force-confold \
		--skip-same-version --install ${archives}
	remaining="$(remove_from "${remaining}" "${names}")"
	installed_predeps="$(printf "%s\n%s" "${installed_predeps}" "${names}")"
done

# Now we can unpack and configure the rest of the "base" package set.
if [ -n "${remaining}" ]; then
	regex="^($(echo "${remaining}" | escape_names | tr '\n' '|'))$"
	remaining="$(filter_packages "${base_packages}" "Package" "${regex}")"
	remaining_archives="$(deb_for_packages "${remaining}" ".*" "${mirror}")"
	# shellcheck disable=SC2086
	in_target "${output_dir}" dpkg --force-overwrite --force-confold \
		--skip-same-version --unpack ${remaining_archives}
fi

in_target "${output_dir}" \
	dpkg --force-confold --skip-same-version --configure -a

# Populate apt lists.
prefix="${output_dir}/var/lib/apt/lists/$( \
	echo "${mirror}/dists/${distribution}" |
	sed -e 's|^https\?://||' -e 's|/|_|g')"
cp -p "${release}" "${prefix}_InRelease"
tail -n +4 "${release}" |
	sed '/^-----BEGIN PGP SIGNATURE-----$/Q' > "${prefix}_Release"
truncate -s -1 "${prefix}_Release"
sed -n '/^-----BEGIN PGP SIGNATURE-----$/,/^-----END PGP SIGNATURE-----$/p' \
	"${release}" > "${prefix}_Release.gpg"
file="main/binary-${architecture}/Packages"
get_release_file "${mirror}" "${distribution}" "${release}" "${file}" \
	"${cache_dir}" "${prefix}_$(echo "${file}" | sed 's|/|_|g')"

echo "# UNCONFIGURED FSTAB FOR BASE SYSTEM" > "${output_dir}/etc/fstab"

# Clean up.
rm -rf "${output_dir}/cache" "${temp_dir}"
umount "${output_dir}/proc"

################################################################################
# Reproducibility Cleanup                                                      #
################################################################################

# The contents of dev are mostly decided by the Linux kernel, and can easily
# be recreated when needed.
rm -rfv "${output_dir:?}"/dev/*

# May appear sometimes.
rm -rfv "${output_dir}"/etc/apparmor.d/local/sbin.dhclient

# Both scripts generate this random ID. Remove it since it can easily be
# regenerated later and is a source of non-reproducibility.
rm -rfv "${output_dir}"/etc/machine-id

# According to the Linux FHS, the /run directory is to be cleared each boot, so
# it should be fine to remove its contents.
# https://refspecs.linuxfoundation.org/FHS_3.0/fhs/ch03s15.html
rm -rfv "${output_dir}"/run/*

# According to the Linux FHS, the application must be able to regenerate or
# restore any data in the /var/cache directory, so it should be fine to
# remove its contents.
rm -rfv "${output_dir}"/var/cache/*

# This is a file used by dpkg to know what packages are available. The local
# ./bootstrap script and debootstrap both generate basically the same file,
# but the ordering is different. They could be made the same with some
# effort, but this file is not actually essential and can be regenerated
# from the package list, so it should be good to remove here.
rm -rfv "${output_dir}"/var/lib/dpkg/available

# This is a file that can be used by dpkg for backup purposes, but is
# generally not needed, especially on a just-bootstrapped system.
rm -rfv "${output_dir}"/var/lib/dpkg/status-old

# These are log files that have timestamps which are a source of
# non-reproducibility.
rm -rfv "${output_dir}"/var/log/dpkg.log
rm -rfv "${output_dir}"/var/log/alternatives.log

# Ensure some timestamps are updated.
touch "${output_dir}"/proc
touch "${output_dir}"/usr/lib/mime
touch "${output_dir}"/usr/lib/systemd
touch "${output_dir}"/usr/lib/terminfo
touch "${output_dir}"/usr/libexec
touch "${output_dir}"/usr/share
touch "${output_dir}"/usr/share/bash-completion
touch "${output_dir}"/usr/share/doc
touch "${output_dir}"/usr/share/lintian
touch "${output_dir}"/usr/share/locale
touch "${output_dir}"/usr/share/locale/*
touch "${output_dir}"/usr/share/man
touch "${output_dir}"/usr/share/man/*
touch "${output_dir}"/usr/share/perl5
touch "${output_dir}"/var/lib/apt
touch "${output_dir}"/var/lib/apt/lists/partial
